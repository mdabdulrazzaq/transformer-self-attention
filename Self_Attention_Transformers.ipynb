{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca40202f",
   "metadata": {},
   "source": [
    "# Understanding Self-Attention in Transformers with Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225a065",
   "metadata": {},
   "source": [
    "Self-attention in transformers is the backbone of transformer models, enabling them to understand relationships between words in a sentence. This blog will explain the concept step-by-step, using a practical example with clear code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c4a1f",
   "metadata": {},
   "source": [
    "## The Setup: Sentence Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4596151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0.5, 0.8, 0.2, 0.6],  # Embedding for \"I\"\n",
    "    [0.7, 0.1, 0.9, 0.3],  # Embedding for \"love\"\n",
    "    [0.4, 0.6, 0.3, 0.8]   # Embedding for \"bears\"\n",
    "])\n",
    "print(\"Matrix X:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732187c",
   "metadata": {},
   "source": [
    "## Step 1: Compute the Scores Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46428c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.dot(X, X.T)\n",
    "print(\"Scores Matrix:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6f85d",
   "metadata": {},
   "source": [
    "## Step 2: Apply Softmax to Get Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18885a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(matrix):\n",
    "    exp_matrix = np.exp(matrix)\n",
    "    return exp_matrix / exp_matrix.sum(axis=0, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scores)\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64aca7",
   "metadata": {},
   "source": [
    "## Step 3: Compute Context-Aware Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf61f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    new_embeddings[i] = (\n",
    "        attention_weights[0, i] * X[0] +\n",
    "        attention_weights[1, i] * X[1] +\n",
    "        attention_weights[2, i] * X[2]\n",
    "    )\n",
    "\n",
    "print(\"New Embeddings:\")\n",
    "print(new_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21ba0f",
   "metadata": {},
   "source": [
    "## Real-World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb124f",
   "metadata": {},
   "source": [
    "In real-world scenarios, transformers work on much larger scales. Instead of processing just a single sentence, they handle entire documents, paragraphs, or sequences with thousands of tokens. Hereâ€™s how it works:\n",
    "\n",
    "1. **Tokenization:** Text is split into smaller pieces (tokens) like words or subwords.\n",
    "2. **Embedding Layers:** Each token is converted into a vector using pre-trained embeddings or learned during training.\n",
    "3. **Multi-Head Self-Attention:** Multiple self-attention mechanisms run in parallel to capture different types of relationships (e.g., semantic, syntactic).\n",
    "4. **Transformer Stacks:** These attention mechanisms are applied across several layers to refine the contextual understanding.\n",
    "5. **Applications:**\n",
    "   - **Language Models:** GPT and BERT generate coherent text and answer questions by understanding the context.\n",
    "   - **Translation:** Models like Google Translate map relationships across languages.\n",
    "   - **Summarization:** Transformers extract key points from lengthy documents efficiently.\n",
    "\n",
    "The scalability of self-attention allows transformers to revolutionize natural language processing (NLP), enabling breakthroughs in AI-driven applications."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
